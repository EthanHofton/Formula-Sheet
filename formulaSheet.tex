\documentclass{article}

\usepackage{amsmath}
\usepackage{amsfonts}

\DeclareMathOperator{\Var}{\textrm{Var}}
\DeclareMathOperator{\Cov}{\textrm{Cov}}

\title{ECM1416 Forumlas}
\author{Ethan Hofton}
\date{\today}

\begin{document}
    \maketitle

    \tableofcontents

    \section{Linear Algeobra}

        \subsection{Vectors}

        \subsubsection{Vector Norm}
        \begin{equation}
            ||v|| = \sqrt{\sum{v^2_i}}
        \end{equation}

        \subsubsection{Inner Product}
        \begin{gather}
            u \cdot v = |u||v|\cos(\theta) \\
            u \cdot v = u^T \cdot v = \sum^{n}_{i=1}{u_iv_i}
        \end{gather}

        \subsubsection{Rotating Vector}
        \begin{equation}
            v' = \begin{bmatrix} v_x \cos(\theta) - v_y \sin(\theta) \\ v_x \sin(\theta) + v_y \cos(\theta) \end{bmatrix}
        \end{equation}

        \subsection{Matracies}

        \subsubsection{Matrix Product ($ A=(m \times n), B=(n \times p) $)}
        \begin{equation}
            A \cdot B = \begin{bmatrix} \Sigma^n_{i=1}{a_{1i}b_{i1}} & \cdots & \Sigma^n_{i=1}{a_{1i}b_{ip}} \\
            \vdots & \ddots & \vdots \\ 
            \Sigma^n_{i=1}{a_{ni}b_{i1}} & \cdots & \Sigma^n_{i=1}{a_{ni}b_{ip}} \end{bmatrix}
        \end{equation}

        \subsubsection {Rotation Matrix}
        \begin{equation}
            \begin{bmatrix} x' \\ y' \end{bmatrix} = \begin{bmatrix} \cos(\theta) & -\sin(\theta) \\ \sin(\theta) & \cos(\theta) \end{bmatrix} \begin{bmatrix} x \\ y \end{bmatrix}
        \end{equation}

        \subsubsection {Scale Matrix}
        \begin{equation}
            \begin{bmatrix} x' \\ y' \end{bmatrix} = \begin{bmatrix} \lambda_1 x \\ \lambda_2 y \end{bmatrix} = \begin{bmatrix} \lambda_1 & 0 \\ 0 & \lambda_2 \end{bmatrix} \begin{bmatrix} x \\ y \end{bmatrix}
        \end{equation}

        \subsubsection {Shear Matrix}
        \begin{equation}
            \begin{bmatrix} x' \\ y' \end{bmatrix} = \begin{bmatrix} x + \lambda y \\ y \end{bmatrix} = \begin{bmatrix} 1 & \lambda \\ 0 & 1 \end{bmatrix} \begin{bmatrix} x \\ y \end{bmatrix}
        \end{equation}
        \begin{equation}
            \begin{bmatrix} x' \\ y' \end{bmatrix} = \begin{bmatrix} x \\ \mu x + y \end{bmatrix} = \begin{bmatrix} 1 & 0 \\ \mu & 1 \end{bmatrix} \begin{bmatrix} x \\ y \end{bmatrix}
        \end{equation}

        \subsubsection {Transpose Properties}
        \begin{equation}
            (A+B)^T=A^T+B^T
        \end{equation}
        \begin{equation}
            (AB)^T=A^TB^T
        \end{equation}

        \subsubsection {Identity Matrix}
        \begin{equation}
            v = I_nv, \forall v \in \mathbb{R}^n
        \end{equation}

        \subsubsection {Determinant of a $2 \times 2$ Matrix}
        \begin{equation}
            \det(\begin{bmatrix} a & b \\ c & d \end{bmatrix}) = ad - bc
        \end{equation}

        \subsubsection {Determinant of a $n \times n$ Matrix}
        \begin{equation}
            \det(A) = \sum^n_{k=1} (-1)^{i+k} a_{ik} M_{ik} = \sum^n_{k=1} (-1)^{j+k} a_{kj} M_{kj}
        \end{equation}

        \subsubsection {Inverse of a $2 \times 2$ Matrix}
        \begin{equation}
            A^-1 = \begin{bmatrix} a & b \\ c & d \end{bmatrix}^{-1} = \frac 1 {\det(A)} \begin{bmatrix} d & -b \\ -c & a\end{bmatrix}
        \end{equation}

        \subsubsection {Inverse of a $n \times n$ Matrix}
        \begin{equation}
            \begin{split}
            A^-1 = \frac 1 {\det(A)} C^T = \frac 1 {\det(A)} \begin{bmatrix} 
                C_{11} & C_{21} & \cdots & C_{n1} \\ 
                C_{21} & C_{22} & \cdots & C_{n2} \\
                \vdots & \vdots & \ddots & \vdots \\
                C_{1n} & C_{2n} & \cdots & C_{nn} \\
            \end{bmatrix} \\
            C_{ij} = (-1)^{i+j}M_{ij}
        \end{split}
        \end{equation}

        \subsubsection {Properties of an Inverse}
        \begin{equation}
            (A^{-1})^{-1} = A
        \end{equation}
        \begin{equation}
            (A^{T})^{-1} = (A^{-1})^T
        \end{equation}
        \begin{equation}
            (AB)^{-1} = B^{-1}A^{-1}
        \end{equation}
        \begin{equation}
            (kA)^{-1} = k^{-1}A^{-1}
        \end{equation}

        \subsubsection {Solving a System of Linear Equations}
        \begin{equation}
            Ax = b 
        \end{equation}
        \begin{equation}
            x = A^{-1}b
        \end{equation}

    \section{Differentation}

        \subsection{Matrix Calculus}

        \subsubsection{Gradient}
        \begin{equation}
            \nabla f = \begin{bmatrix} 
                f_{x_1} \\
                f_{x_2} \\
                \vdots \\
                f_{x_n} \\
            \end{bmatrix}
        \end{equation}

        \subsubsection{Hessian}
        \begin{equation}
            H_f = \begin{bmatrix} 
                f_{x_1x_1} & f_{x_1x_2} & \hdots & f_{x_1x_n} \\
                f_{x_2x_1} & f_{x_2x_2} & \hdots & f_{x_2x_n} \\
                \vdots & \vdots & \ddots & \vdots \\
                f_{x_mx_1} & f_{x_mx_2} & \hdots & f_{x_mx_n} \\
            \end{bmatrix}
        \end{equation}

        \subsubsection{Jacobian}
        \begin{equation}
            D_f = \begin{bmatrix} 
                \nabla f_1 \\
                \nabla f_2 \\
                \vdots \\
                \nabla f_n \\
            \end{bmatrix} = \begin{bmatrix}
                f_{1x_1} & f_{1x_2} & \hdots & f_{1x_n} \\
                f_{2x_1} & f_{2x_2} & \hdots & f_{2x_n} \\
                \vdots & \vdots & \ddots & \vdots  \\
                f_{nx_1} & f_{nx_2} & \hdots & f_{nx_n} \\
            \end{bmatrix}
        \end{equation}

        \subsection{Taylor Expantion}

        \begin{equation}
            f(x) \approx \sum_{i=0}^n \frac {f^i(a)(x-a)^i} {i!}
        \end{equation}

        \subsection{Maclorian Expantion (a = 0)}

        \subsubsection{$\sin(x)$}
        \begin{equation}
            \sin(x) \approx \sum_{k=0}^{n} \frac {(-1)^kx^{2k+1}} {(2k+1)!}
        \end{equation}

            
        \subsubsection{$\cos(x)$}
        \begin{equation}
            \cos(x) \approx \sum_{k=0}^{n} \frac {(-1)^kx^{2k}} {(2k)!}
        \end{equation}

        \subsubsection{$e^x$}
        \begin{equation}
            e^x \approx \sum_{k=0}^n \frac {x^k} {k!}
        \end{equation}

        \subsubsection{$\ln|1+x|$}
        \begin{equation}
            \ln|1+x| \approx \sum_{k=0}^n (-1)^{k} \frac {x^{k+1}} {k+1}
        \end{equation}

        \subsubsection{$c(\frac {1} {1-x})$}
        \begin{equation}
            c(\frac {1} {1-x}) \approx c \sum_{k=0}^{n} x^k
        \end{equation}

        \subsection{Solving ODEs}

        \subsubsection{Intergrating Factor}
        \begin{equation}
        \begin{split}
            \frac {dy} {dx} + P(x)y = Q(x) \\
            \textrm{I.F} = e^{\int P(x) dx} \\
            \frac d {dx} (e^{\int P(x) dx}y) = Q(x)e^{\int P(x) dx} \\
            y = \frac {\int Q(x)e^{\int P(x) dx}dx} {e^{\int P(x) dx}}
        \end{split}
        \end{equation}

        \subsubsection{Seperation of Variables}
        \begin{equation}
        \begin{split}
            \frac {dy} {dx} = g(x)h(y) \\
            \frac {dy} {dx} \frac 1 {h(y)} = g(x) \\
            \int \frac 1 {h(y)} dy = \int g(x) dx
        \end{split}
        \end{equation}

        \subsubsection{Eulers Method}
        \begin{equation}
            \begin{split}
            y_{n+1} = y_n + hf(x_n, y+n) \\
            x_{n+1} = x_n + h
        \end{split}
        \end{equation}
        

    \section{Probability}

    \subsection{Conditional Probability}

    \subsubsection{Given That}
    \begin{equation}
        P(A | B) = \frac {P(A \cap B)} {P(B)}
    \end{equation}

    \subsubsection{Independence}
    \begin{equation}
        P(A \cap B) = P(A)P(B)
    \end{equation}

    \subsubsection{Law of Total Probability}
    \begin{equation}
        P(A) = \sum_n P(A \cap B_n) = \sum_n P(A | B_n)P(B_n) = \sum_n P(B_n | A)P(A)
    \end{equation}

    \subsubsection{Bayes Theorm}
    \begin{gather}
        P(A | B) = \frac {P(B | A)P(A)} { P(B) } \\
        P(A | X) = \frac {P(X |A )P(A)} { P(X) } = \frac {P(X |A )P(A)} { P(X | A)P(A) + P(X | A^C)P(A^C) }
    \end{gather}

    \subsection{Random Variables}

    \subsubsection{Descrete Random Variables}
    \begin{equation}
    \begin{split}
        P(x_i) = P(X = x_i) \\
        P(X=x_i) \geq 0, \forall i \\
        \sum P(X=x_i) = 1
    \end{split}
    \end{equation}

    \subsubsection{Continus Random Variables}
    \begin{equation}
    \begin{split}
        P(a \leq X \leq b) = \int_a^b {f(x)dx} \\
        f(x) \geq 0 , \forall x \in X \\
        \int_{\Omega} f(x) dx = 1 \\
        f(x) = 0, x \notin X
    \end{split}
    \end{equation}
    \begin{equation}
    \begin{split}
        P(a \leq X \leq b) = P(a < X \leq b) \\
        = P(a \leq X < b) \\
        = P(a < X < b)
    \end{split}
    \end{equation}


    \subsection{Cumulative Distrubution Function}
    \subsubsection{General Case}
    \begin{equation}
        F(x) = P(X \leq x)
    \end{equation}

    \subsubsection{X is Discrete}
    \begin{equation}
        F(x) = P(X \leq x) = \sum_{\forall x_i \leq x} P(x_i)
    \end{equation}

    \subsubsection{X is Continus}
    \begin{gather}
        F(x) = P(X \leq x) = \int_{-\infty}^{x} f(t) dt \\
        P(a \leq X \leq b) = F(b) - F(a)
    \end{gather}

    \subsubsection{Properties of CDF}
    \begin{gather}
        0 \leq F(x) \leq 1 \\
        x < y \rightarrow F(x) \leq F(y) \\
        \lim_{x \to \infty} F(x) = 1 \\
        \lim_{x \to -\infty} F(x) = 0
    \end{gather}

    \subsection{Mean, expectation}

    \subsubsection{First Moment of X}
    \begin{equation}
        \mu = E[x] = \left.
            \begin{cases}
                \sum^n_{i=1}x_iP(X=x_i) & \textrm{Discrete R.V} \\
                \int_{-\infty}^{\infty}xf(x)dx & \textrm{Continus R.V}
            \end{cases}
        \right.
    \end{equation}

    \subsubsection{$n$th Moment of X}
    \begin{equation}
        E[x^n] = \left.
            \begin{cases}
                \sum^N_{i=1}x_i^nP(X=x_i) & \textrm{Discrete R.V} \\
                \int_{-\infty}^{\infty}x^nf(x)dx & \textrm{Continus R.V}
            \end{cases}
        \right.
    \end{equation}

    \subsubsection{Properties of Expectation}
    \begin{gather}
        x \geq 0 \iff E[x] \geq 0 \\
        E[cx] = cE[x] \\
        E[x+y] = E[x] + E[y]
    \end{gather}

    \subsubsection{Variance}
    \begin{gather}
        \Var(x) = \sigma^2 = E[x^2] - (E[x])^2
    \end{gather}

    \subsubsection{Standard Deviation}
    \begin{gather}
        \sigma = \sqrt{\Var(x)} \\
        \Var(xC) = c^2\Var(x) \\
        \Var(x+y) = \Var(x) + \Var(y) \qquad \textrm{if } x,y \ \textrm{are indepedent} \\
        \Var(x+y) = \Var(x) + \Var(y) + 2\Cov(x, y) \qquad \textrm{if } x,y \ \textrm{are dependant} \\
    \end{gather}

    \subsubsection{Covariance}
    \begin{gather}
        \Cov(x,y) = E[xy] - E[x]E[y] \\
        \Cov(x,x) = E[x^2] - (E[x])^2 = \Var(x) \\
        \Cov(x,y) = 0 \qquad x,y \ \textrm{are indepedent}
    \end{gather}

    \section{Markov Chains}

\end{document}
